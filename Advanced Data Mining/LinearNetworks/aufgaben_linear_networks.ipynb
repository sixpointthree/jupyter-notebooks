{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben Linear Networks\n",
    "========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the common imports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you should create a linear regression\n",
    "model from scratch and test it on some synthetically created data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "n_samples=100\n",
    "X, y = synthetic_data(true_w, true_b, n_samples)\n",
    "K = 2\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to fit a simple regression model with Batch Gradient Descent.\n",
    "We start with randomly chosen values for the weights and zero bias.\n",
    "First, implement the function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "print(X.shape)\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the loss functions to be used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    \"\"\" Compute the sum of the quadratic errors \"\"\"\n",
    "    assert(len(y_hat) == len(y))\n",
    "    squares = torch.zeros(0, 0)\n",
    "    squares.to(torch.float64)\n",
    "    for i in range(0, len(y_hat)):\n",
    "        squares = (y_hat[i] - y[i])**2\n",
    "    return squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the training loop for Gradient Descent.\n",
    "You should not use `autograd` for computing the gradient, instead\n",
    "build on the closed formula presented in the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights: tensor([ 2.0000, -3.4000])\n",
      "Epoch(0):\n",
      "  - Gradients: tensor([-1.4493,  3.1132])\n",
      "  - New Weights: tensor([[ 0.1603],\n",
      "        [-0.3143]])\n",
      "Epoch(10):\n",
      "  - Gradients: tensor([-0.7525,  1.3919])\n",
      "  - New Weights: tensor([[ 1.1906],\n",
      "        [-2.3669]])\n",
      "Epoch(20):\n",
      "  - Gradients: tensor([-0.3845,  0.6252])\n",
      "  - New Weights: tensor([[ 1.7210],\n",
      "        [-3.2866]])\n",
      "Epoch(30):\n",
      "  - Gradients: tensor([-0.1941,  0.2821])\n",
      "  - New Weights: tensor([[ 1.9904],\n",
      "        [-3.7007]])\n",
      "Epoch(40):\n",
      "  - Gradients: tensor([-0.0971,  0.1278])\n",
      "  - New Weights: tensor([[ 2.1258],\n",
      "        [-3.8879]])\n",
      "Epoch(50):\n",
      "  - Gradients: tensor([-0.0482,  0.0582])\n",
      "  - New Weights: tensor([[ 2.1932],\n",
      "        [-3.9729]])\n",
      "Epoch(60):\n",
      "  - Gradients: tensor([-0.0238,  0.0266])\n",
      "  - New Weights: tensor([[ 2.2266],\n",
      "        [-4.0117]])\n",
      "Epoch(70):\n",
      "  - Gradients: tensor([-0.0117,  0.0122])\n",
      "  - New Weights: tensor([[ 2.2430],\n",
      "        [-4.0295]])\n",
      "Epoch(80):\n",
      "  - Gradients: tensor([-0.0057,  0.0056])\n",
      "  - New Weights: tensor([[ 2.2510],\n",
      "        [-4.0376]])\n",
      "Epoch(90):\n",
      "  - Gradients: tensor([-0.0028,  0.0026])\n",
      "  - New Weights: tensor([[ 2.2550],\n",
      "        [-4.0414]])\n",
      "Final weights: tensor([[ 2.2567],\n",
      "        [-4.0430]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x166e4869100>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch) # to record current loss\n",
    "print(f\"True weights: {true_w}\")\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[epoch] = squared_loss(y_hat, y)\n",
    "    # 2. Use y_hat and y to compute the gradients\n",
    "    gradient_vals = torch.zeros(len(X), len(w))\n",
    "    for i in range(0, len(X)):\n",
    "        gradient_vals[i] = (y_hat[i] - y[i]) * X[i]\n",
    "    gradients = torch.mean(gradient_vals, 0)\n",
    "    # 3. Update the parameters\n",
    "    w = w - step * gradients.reshape(-1, 1)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch({epoch}):\")\n",
    "        print(f\"  - Gradients: {gradients}\")\n",
    "        print(f\"  - New Weights: {w}\")\n",
    "print(f\"Final weights: {w}\")\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear networks with autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to use `autograd` the compute the gradient.\n",
    "You can use the same skeleton as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0154],\n",
      "        [-0.0029]], requires_grad=True)\n",
      "<class 'torch.Tensor'>\n",
      "Epoch(0):\n",
      "  - Gradients: tensor([-0.9225, -2.1312])\n",
      "  - New Weights: tensor([[0.1077],\n",
      "        [0.2102]], grad_fn=<SubBackward0>)\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [138]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m loss_arr[epoch] \u001b[38;5;241m=\u001b[39m squared_loss(y_hat, y)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(loss_arr[epoch]))\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 2. Use the computed loss to compute the gradients\u001b[39;00m\n\u001b[0;32m     19\u001b[0m gradients \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K,1), requires_grad=True)\n",
    "print(w)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = torch.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[epoch] = squared_loss(y_hat, y)\n",
    "    print(type(loss_arr[epoch]))\n",
    "    loss_arr[epoch].backward()\n",
    "    # 2. Use the computed loss to compute the gradients\n",
    "    gradients = torch.zeros(2)\n",
    "    for i in range(0, len(w)):\n",
    "        gradients[i] = w.grad[i]\n",
    "    # 3. Update the parameters, remember to zero the gradients\n",
    "    w = w - step * gradients.reshape(-1, 1)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch({epoch}):\")\n",
    "        print(f\"  - Gradients: {gradients}\")\n",
    "        print(f\"  - New Weights: {w}\")\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  want to implement a linear network for classification.\n",
    "We use the famous IRIS data set as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the network is implemented as a class,\n",
    "the only thing missing is the implementation of the softmax function,\n",
    "for example\n",
    "$$\n",
    "softmax(y)_1 = \\frac{e^{y_1}}{ \\sum_{i=1}^p  e^{y_i} }.\n",
    "$$\n",
    "You have to implement it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3329],\n",
      "        [0.3359],\n",
      "        [0.3312]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def softmax(y):\n",
    "    y_out = torch.zeros(len(y), 1, dtype=torch.float64)    \n",
    "    y_sum = torch.zeros(1, 1, dtype=torch.float64)\n",
    "    for i in range(0, len(y_out)):\n",
    "        y_sum += torch.exp(y[i])\n",
    "    for i in range(0, len(y_out)):\n",
    "        y_out[i] = torch.exp(y[i]) / y_sum\n",
    "    return y_out\n",
    "\n",
    "print(softmax(torch.normal(0, 0.01, size=(3, 1), dtype=torch.float64)))\n",
    "\n",
    "class SoftmaxNetwork:\n",
    "\n",
    "    def __init__(self, num_input, num_output, dtype=torch.float64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_input: dimension of input space\n",
    "            num_output: number if output classes\n",
    "        \"\"\"\n",
    "        self.w = torch.randn((num_input,num_output),\n",
    "                             dtype=dtype).requires_grad_(True)\n",
    "        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: tensor of shape (n, d)\n",
    "        \"\"\"\n",
    "        y = (X @ self.w + self.b)\n",
    "        return softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to implement the cross entropy loss, it is already finished:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation does not require a one-hot-encoding for $y$\n",
    "(but there is one side effect: $y$ has to be of type `torch.int64`!).\n",
    "\n",
    "The final step is to implement a function that runs the training for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net, X, y, f_loss, n_epochs, lr=0.1):\n",
    "    \"\"\" Run the training.\n",
    "    Args:\n",
    "       net: an instance of SoftmaxNetwork\n",
    "       X, y: training data\n",
    "       f_loss: the loss function\n",
    "       n_epochs: number of epochs\n",
    "       lr: the learning rate\n",
    "    \n",
    "    Returns:\n",
    "      training loss: np.array (loss per epoch)\n",
    "    \"\"\"\n",
    "    for epoch in range(0, n_epochs):\n",
    "        y_hat = net.forward(X)\n",
    "        loss = f_loss(y_hat, y)\n",
    "        # no idea how to do shit with this autograd \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model, don't forget to cast X and y to Pytorch tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 4])\n",
      "torch.Size([150, 1])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 1] doesn't match the broadcast shape [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [166]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[0;32m      8\u001b[0m net \u001b[38;5;241m=\u001b[39m SoftmaxNetwork(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss)\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [165]\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(net, X, y, f_loss, n_epochs, lr)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\" Run the training.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m   net: an instance of SoftmaxNetwork\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m  training loss: np.array (loss per epoch)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_epochs):\n\u001b[1;32m---> 14\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m f_loss(y_hat, y)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# no idea how to do shit with this autograd \u001b[39;00m\n",
      "Input \u001b[1;32mIn [163]\u001b[0m, in \u001b[0;36mSoftmaxNetwork.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    X: tensor of shape (n, d)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m y \u001b[38;5;241m=\u001b[39m (X \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [163]\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m      3\u001b[0m y_sum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(y_out)):\n\u001b[1;32m----> 5\u001b[0m     y_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(y[i])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(y_out)):\n\u001b[0;32m      7\u001b[0m     y_out[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(y[i]) \u001b[38;5;241m/\u001b[39m y_sum\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 1] doesn't match the broadcast shape [1, 3]"
     ]
    }
   ],
   "source": [
    "# TODO: cast X and y\n",
    "X = torch.from_numpy(X)\n",
    "print(X.shape)\n",
    "y = torch.reshape(torch.from_numpy(y), (-1, 1))\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "net = SoftmaxNetwork(4,3)\n",
    "train_loss = run_training(net, X, y,  cross_entropy, n_epochs=100, lr=0.2)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Learning curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Run the training several times, and observe the different learning curves.\n",
    "2.  Try the same with a lower learning rate, say $lr=0.05$. Do you see any differences?\n",
    "\n",
    "Finally check the accuracy of the model, that is the fraction of correctly predicted examples.\n",
    "Of course this is on training only. If you like you can try to split the\n",
    "data into train and test and evaluate your network on the test data set.\n",
    "A useful function for this is `train_test_split` found in `sklearn.model_selection`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "org": null,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
