{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben Linear Networks\n",
    "========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the common imports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you should create a linear regression\n",
    "model from scratch and test it on some synthetically created data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    # Rauschen dazu addieren, Form von y\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "n_samples=100\n",
    "X, y = synthetic_data(true_w, true_b, n_samples)\n",
    "K = 2\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to fit a simple regression model with Batch Gradient Descent.\n",
    "We start with randomly chosen values for the weights and zero bias.\n",
    "First, implement the function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "print(X.shape)\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the loss functions to be used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    \"\"\" Compute the sum of the quadratic errors \"\"\"\n",
    "    assert(len(y_hat) == len(y))\n",
    "    squares = torch.zeros(0, 0)\n",
    "    squares.to(torch.float64)\n",
    "    for i in range(0, len(y_hat)):\n",
    "        squares = (y_hat[i] - y[i])**2\n",
    "    return squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the training loop for Gradient Descent.\n",
    "You should not use `autograd` for computing the gradient, instead\n",
    "build on the closed formula presented in the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights: tensor([ 2.0000, -3.4000])\n",
      "Epoch(0):\n",
      "  - Gradients: tensor([-1.3542,  3.0331])\n",
      "  - New Weights: tensor([[ 0.1508],\n",
      "        [-0.3062]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(10):\n",
      "  - Gradients: tensor([-0.7343,  1.1132])\n",
      "  - New Weights: tensor([[ 1.1425],\n",
      "        [-2.1220]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(20):\n",
      "  - Gradients: tensor([-0.3669,  0.4226])\n",
      "  - New Weights: tensor([[ 1.6564],\n",
      "        [-2.7991]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(30):\n",
      "  - Gradients: tensor([-0.1754,  0.1656])\n",
      "  - New Weights: tensor([[ 1.9071],\n",
      "        [-3.0601]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(40):\n",
      "  - Gradients: tensor([-0.0817,  0.0668])\n",
      "  - New Weights: tensor([[ 2.0253],\n",
      "        [-3.1638]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(50):\n",
      "  - Gradients: tensor([-0.0375,  0.0276])\n",
      "  - New Weights: tensor([[ 2.0799],\n",
      "        [-3.2061]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(60):\n",
      "  - Gradients: tensor([-0.0170,  0.0116])\n",
      "  - New Weights: tensor([[ 2.1048],\n",
      "        [-3.2237]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(70):\n",
      "  - Gradients: tensor([-0.0077,  0.0050])\n",
      "  - New Weights: tensor([[ 2.1161],\n",
      "        [-3.2312]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(80):\n",
      "  - Gradients: tensor([-0.0034,  0.0021])\n",
      "  - New Weights: tensor([[ 2.1211],\n",
      "        [-3.2344]])\n",
      "  - New Bias: tensor([0.])\n",
      "Epoch(90):\n",
      "  - Gradients: tensor([-0.0015,  0.0009])\n",
      "  - New Weights: tensor([[ 2.1234],\n",
      "        [-3.2358]])\n",
      "  - New Bias: tensor([0.])\n",
      "Final weights: tensor([[ 2.1243],\n",
      "        [-3.2364]])\n",
      "Final bias: tensor([0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x260cc3a1280>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch) # to record current loss\n",
    "print(f\"True weights: {true_w}\")\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[epoch] = squared_loss(y_hat, y)\n",
    "    # 2. Use y_hat and y to compute the gradients\n",
    "    gradient_vals = torch.zeros(len(X), len(w))\n",
    "    #bias erg√§nzen!!!!!!!!!!!!\n",
    "    for i in range(0, len(X)):\n",
    "        gradient_vals[i] = (y_hat[i] - y[i]) * X[i]\n",
    "    gradients = torch.mean(gradient_vals, 0)\n",
    "    # 3. Update the parameters\n",
    "    w = w - step * gradients.reshape(-1, 1)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch({epoch}):\")\n",
    "        print(f\"  - Gradients: {gradients}\")\n",
    "        print(f\"  - New Weights: {w}\")\n",
    "        print(f\"  - New Bias: {b}\")\n",
    "print(f\"Final weights: {w}\")\n",
    "print(f\"Final bias: {b}\")\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear networks with autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to use `autograd` the compute the gradient.\n",
    "You can use the same skeleton as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0154],\n",
      "        [-0.0029]], requires_grad=True)\n",
      "Epoch(0):\n",
      "  - Gradients: tensor([0.6448, 7.8065])\n",
      "  - New Weights: tensor([[-0.0491],\n",
      "        [-0.7836]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steffen.lang\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m gradients \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(w)):\n\u001b[1;32m---> 21\u001b[0m     gradients[i] \u001b[38;5;241m=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     22\u001b[0m     w\u001b[38;5;241m.\u001b[39mgrad[i]\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 3. Update the parameters, remember to zero the gradients\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K,1), requires_grad=True)\n",
    "print(w)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = torch.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "    # remember the loss for plotting it later\n",
    "    loss = squared_loss(y_hat, y)\n",
    "    loss_arr[epoch] = loss.detach()\n",
    "    loss.backward()\n",
    "    # 2. Use the computed loss to compute the gradients\n",
    "    gradients = torch.zeros(2)\n",
    "    for i in range(0, len(w)):\n",
    "        gradients[i] = w.grad[i]\n",
    "        w.grad[i].zero_()\n",
    "    # 3. Update the parameters, remember to zero the gradients\n",
    "    w = w - step * gradients.reshape(-1, 1)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch({epoch}):\")\n",
    "        print(f\"  - Gradients: {gradients}\")\n",
    "        print(f\"  - New Weights: {w}\")\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  want to implement a linear network for classification.\n",
    "We use the famous IRIS data set as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the network is implemented as a class,\n",
    "the only thing missing is the implementation of the softmax function,\n",
    "for example\n",
    "$$\n",
    "softmax(y)_1 = \\frac{e^{y_1}}{ \\sum_{i=1}^p  e^{y_i} }.\n",
    "$$\n",
    "You have to implement it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def softmax(y):\n",
    "    y_exp = torch.exp(y)\n",
    "    partition = y_exp.sum(axis=1, keepdims=True)\n",
    "    return y_exp / partition\n",
    "\n",
    "print(softmax(torch.normal(0, 0.01, size=(3, 1), dtype=torch.float64)))\n",
    "\n",
    "class SoftmaxNetwork:\n",
    "\n",
    "    def __init__(self, num_input, num_output, dtype=torch.float64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_input: dimension of input space\n",
    "            num_output: number if output classes\n",
    "        \"\"\"\n",
    "        self.w = torch.randn((num_input,num_output),\n",
    "                             dtype=dtype).requires_grad_(True)\n",
    "        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: tensor of shape (n, d)\n",
    "        \"\"\"\n",
    "        y = (X @ self.w + self.b)\n",
    "        return softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to implement the cross entropy loss, it is already finished:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation does not require a one-hot-encoding for $y$\n",
    "(but there is one side effect: $y$ has to be of type `torch.int64`!).\n",
    "\n",
    "The final step is to implement a function that runs the training for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net, X, y, f_loss, n_epochs, lr=0.1):\n",
    "    \"\"\" Run the training.\n",
    "    Args:\n",
    "       net: an instance of SoftmaxNetwork\n",
    "       X, y: training data\n",
    "       f_loss: the loss function\n",
    "       n_epochs: number of epochs\n",
    "       lr: the learning rate\n",
    "    \n",
    "    Returns:\n",
    "      training loss: np.array (loss per epoch)\n",
    "    \"\"\"\n",
    "    for epoch in range(0, n_epochs):\n",
    "        y_hat = net.forward(X)\n",
    "        loss = f_loss(y_hat, y)\n",
    "        # no idea how to do shit with this autograd \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model, don't forget to cast X and y to Pytorch tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 4])\n",
      "torch.Size([150, 1])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[0;32m      8\u001b[0m net \u001b[38;5;241m=\u001b[39m SoftmaxNetwork(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss)\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(net, X, y, f_loss, n_epochs, lr)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_epochs):\n\u001b[0;32m     14\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mf_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# no idea how to do shit with this autograd \u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(y_hat, y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy\u001b[39m(y_hat, y):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(\u001b[43my_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m))\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "# TODO: cast X and y\n",
    "X = torch.from_numpy(X)\n",
    "print(X.shape)\n",
    "y = torch.reshape(torch.from_numpy(y), (-1, 1))\n",
    "print(y.shape)\n",
    "print(y)\n",
    "\n",
    "net = SoftmaxNetwork(4,3)\n",
    "train_loss = run_training(net, X, y,  cross_entropy, n_epochs=100, lr=0.2)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Learning curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Run the training several times, and observe the different learning curves.\n",
    "2.  Try the same with a lower learning rate, say $lr=0.05$. Do you see any differences?\n",
    "\n",
    "Finally check the accuracy of the model, that is the fraction of correctly predicted examples.\n",
    "Of course this is on training only. If you like you can try to split the\n",
    "data into train and test and evaluate your network on the test data set.\n",
    "A useful function for this is `train_test_split` found in `sklearn.model_selection`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "org": null,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
