{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: trump.model\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm \n",
    "import requests\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "files = {\n",
    "    \"trump.model\": \"https://downloads.codingcoursestv.eu/037%20-%20neuronale%20netze/trump.model\",\n",
    "}\n",
    "\n",
    "for local, remote in files.items():\n",
    "    if not os.path.exists(local):\n",
    "        # Streaming, so we can iterate over the response.\n",
    "        r = requests.get(remote, stream=True)\n",
    "\n",
    "        # Total size in bytes.\n",
    "        total_size = int(r.headers.get('content-length', 0));\n",
    "\n",
    "        print(\"Downloading \" + str(total_size / 1024 / 1024) + \" MB.\")\n",
    "\n",
    "        block_size = 1024\n",
    "        with open(local, 'wb') as f:\n",
    "            for data in tqdm(r.iter_content(block_size), total=math.ceil(total_size//block_size), unit='KB', unit_divisor=1024, unit_scale=True):\n",
    "                f.write(data)\n",
    "    \n",
    "    print(\"Done: \" + local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt\n",
    "\n",
    "with open(\"speeches.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    contents = file.read()\n",
    "    \n",
    "contents = contents.split(\"\\n\")\n",
    "contents = [line.strip() for line in contents if \"SPEECH\" not in line]\n",
    "\n",
    "contents = \"\\n\".join(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"word_to_int.trump.pickle\", \"rb\") as file:\n",
    "    word_to_int = pickle.load(file)\n",
    "    \n",
    "with open(\"int_to_word.trump.pickle\", \"rb\") as file:\n",
    "    int_to_word = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens = word_tokenize(contents)\n",
    "\n",
    "tokens_transformed = [word_to_int[word] for word in tokens if word in word_to_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"trump.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want to work , they want to make the country great . I love the people of Iowa . So that 's the way it is . Very simple . With that said , our country is really headed in\n"
     ]
    }
   ],
   "source": [
    "sentence = tokens_transformed[100:140]\n",
    "\n",
    "print(\" \".join([int_to_word[token] for token in sentence]).replace(\"\\\\n\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5845, 6508, 5187, 2247, 3961, 6559, 2570, 4444,   24, 1017, 7036,\n",
       "       5838, 6650,   24, 1797, 7036, 5575, 3961, 6559, 3938, 6499, 4848,\n",
       "       2175, 5187, 4499,   24,  964, 7036, 6778, 2411, 3334, 4758, 2209,\n",
       "         24,    6, 1676,  964, 7036, 4686, 4135])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the things and we 're ; gave back a victory , big immigration inside . And the probably bombers running . Everybody wanted for Republicans , OK , I say , `` No , he doesn ’ t want to wait . The other day did me stupid politicians ? No direction . Jobs , big class before . And so you see the 100 % and clear at the people in peace , in Wisconsin , 45 % and vigilant , is he they do hundreds , and some , recently , we came out and unless you ever say -- the region that you think is a billion stuff ; we didn ’ t mention leadership . We have made a truly while everything I 'm in the fun . I said that `` I really want to have to attack with the middle . You hear these thousands and 18 % – if that was [ the Bobbsey government . That ’ s going to be unpredictable . Because we ’ ve got to take over all the own , so you folks that ’ s tough . Right ? They all think , that ’ s better to totally in Washington and there have no change , weren ’ t getting ISIS into them . I may want to have credit because one of them . It ’ s how that we need . I mean -- I have a mainstream property , wherever I am , `` To are blown to the signal , I said these people are going to bring jobs . It ’ s true . They ’ re going to get their money and people know . I ’ ve been doing me anything . '' So I ’ m here "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence = np.array(tokens_transformed[100:140])\n",
    "\n",
    "for i in range(0, 300):\n",
    "    prediction = model.predict(sentence.reshape(1, 40))\n",
    "    \n",
    "    # word = np.argmax(prediction[0])\n",
    "    word = np.random.choice(len(int_to_word), p=prediction[0])\n",
    "    print(int_to_word[word], end=\" \")\n",
    "    \n",
    "    sentence = np.append(sentence[1:], [word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(int_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
